"""
Â©Copyright 2020 University of Florida Research Foundation, Inc. All rights reserved.
Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).
"""
import logging
import functools

import torch

import kernet.utils as utils
from kernet.models import Flatten
from kernet.layers.klinear import _kLayer
from kernet.models.base_model import BaseModel


logger = logging.getLogger()

# TODO trainable centers?


class kLeNet5(BaseModel):
    def __init__(self, opt, *args, **kwargs):
        super(kLeNet5, self).__init__(*args, **kwargs)
        if opt.dataset in ['mnist', 'fashionmnist']:
            self.feat_len = 400
            in_channels = 1
        elif opt.dataset in ['cifar10', 'cifar100', 'svhn']:
            self.feat_len = 576
            in_channels = 3
        else:
            raise NotImplementedError()

        if opt.activation == 'tanh':
            self.act = torch.nn.Tanh
            self.kernel = 'nn_tanh'
            self.evaluation = 'direct'
        elif opt.activation == 'sigmoid':
            self.act = torch.nn.Sigmoid
            self.kernel = 'nn_sigmoid'
            self.evaluation = 'direct'
        elif opt.activation == 'relu':
            self.act = torch.nn.ReLU
            self.kernel = 'nn_relu'
            self.evaluation = 'direct'
        elif opt.activation == 'gaussian':
            self.act = torch.nn.Tanh
            self.kernel = 'gaussian'
            self.evaluation = 'indirect'
        else:
            raise NotImplementedError()

        conv1 = torch.nn.Conv2d(in_channels, 6, 5, padding=2)
        act1 = self.act()
        pool1 = torch.nn.MaxPool2d(2)
        self.conv1 = torch.nn.Sequential(*[conv1, act1, pool1])
        conv2 = torch.nn.Conv2d(6, 16, 5)
        act2 = self.act()
        pool2 = torch.nn.MaxPool2d(2)
        flatten = Flatten()
        self.conv2 = torch.nn.Sequential(*[conv2, act2, pool2, flatten])

        self.opt = opt

    def forward(self, input, update_centers=True):
        if update_centers:
            self.update_centers()
        output = self.conv1(input)
        output = self.conv2(output)
        output = self.fc1(output)
        output = self.fc2(output)
        output = self.fc3(output)

        return output

    def split(self, n_parts, **kwargs):
        """
        Split the model into n_parts parts for modular training.
        The last part is always the (kernelized) output layer.

        Args:
          n_parts (int): The number of parts to split.

        Returns:
          models (tuple): A tuple of n_parts models.
          trainables (tuple): A tuple of n_parts sets of trainable params.
        """
        if n_parts < 1:
            raise ValueError(
                'n_parts should be at least 1, got {} instead'.format(n_parts))
        if n_parts > 5:
            logger.warning('{} can be split into at most 5 parts. Splitting it into ' +
                           '5 parts instead of the requested {} parts...'.format(self.__class__.__name__, n_parts))
            n_parts = 5

        # in modular training (assumed training mode given split has been called),
        # update_centers do not need to be performed at each forward call. Define
        # new forward to bypass the forward function defined for the entire model
        self.forward = functools.partial(self.forward, update_centers=False)

        output_layer = list(self.children())[-1]
        if n_parts == 1:
            models, trainables = (self,), (self,)
        elif n_parts == 2:
            hidden_layers = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-1]), self.opt)
            t1 = hidden_layers
            models, trainables = (hidden_layers, self), (t1, output_layer)
        elif n_parts == 3:
            hidden_layers1 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-2]), self.opt)
            hidden_layers2 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-1]), self.opt)
            t1 = hidden_layers1
            t2 = hidden_layers2[-1]
            models, trainables = (hidden_layers1, hidden_layers2, self), \
                                 (t1, t2, output_layer)
        elif n_parts == 4:
            hidden_layers1 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-3]), self.opt)
            hidden_layers2 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-2]), self.opt)
            hidden_layers3 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-1]), self.opt)
            t1 = hidden_layers1
            t2 = hidden_layers2[-1]
            t3 = hidden_layers3[-1]
            models, trainables = (hidden_layers1, hidden_layers2, hidden_layers3, self), \
                                 (t1, t2, t3, output_layer)
        elif n_parts == 5:
            hidden_layers1 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-4]), self.opt)
            hidden_layers2 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-3]), self.opt)
            hidden_layers3 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-2]), self.opt)
            hidden_layers4 = utils.attach_head(
                torch.nn.Sequential(*list(self.children())[:-1]), self.opt)
            t1 = hidden_layers1
            t2 = hidden_layers2[-1]
            t3 = hidden_layers3[-1]
            t4 = hidden_layers4[-1]
            models, trainables = (hidden_layers1, hidden_layers2, hidden_layers3, hidden_layers4, self), \
                                 (t1, t2, t3, t4, output_layer)
        else:
            raise ValueError('Invalid n_parts: {}'.format(n_parts))

        logger.debug('Splitting {} into:'.format(self.__class__.__name__))
        for i, t in enumerate(trainables):
            logger.debug('part {}:\n'.format(i + 1) + str(t))

        return models, [_.parameters() for _ in trainables]

    @staticmethod
    def modify_commandline_options(parser, **kwargs):
        parser = _kLayer.modify_commandline_options(parser, **kwargs)
        return parser
